import os
import torch
import numpy as np
import yaml
import pickle
from typing import Optional, Dict, Callable
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.buffers import ReplayBuffer
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor
from stable_baselines3.common.utils import set_random_seed

from DRL_market.common.config import load_config
from DRL_market.common.logging_utils import get_logger
from DRL_market.env.high_level import HighLevelEnv
from DRL_market.common.paths import RunPaths  # [æ–°å¢ž] å¼•å…¥è·¯å¾„ç®¡ç†

# é²æ£’å¯¼å…¥
try:
    from DRL_market.core.math_utils.stats import RewardNormalizer
except ImportError:
    try:
        from DRL_market.core.math.stats import RewardNormalizer
    except ImportError:
        from core.math.stats import RewardNormalizer

from DRL_market.core.constraints.manager import ConstraintManager
from DRL_market.models.policy import CustomMarketExtractor


# [ä¿®æ”¹] å¹¶è¡ŒçŽ¯å¢ƒå·¥åŽ‚å‡½æ•°ï¼šæ”¯æŒä¼ å…¥ Low-Level æ¨¡åž‹è·¯å¾„
def make_parallel_env(cfg, rank: int, seed: int = 0, low_level_path: str = None) -> Callable:
    """
    ç”¨äºŽ SubprocVecEnv çš„å·¥åŽ‚å‡½æ•°ã€‚
    ä¸ºæ¯ä¸ªå­è¿›ç¨‹è®¾ç½®ç‹¬ç«‹çš„éšæœºç§å­ï¼Œå¹¶åœ¨è¿›ç¨‹å†…éƒ¨åŠ è½½ Low-Level æ¨¡åž‹ã€‚
    """

    def _init():
        # [å…³é”®ä¿®å¤] åœ¨å­è¿›ç¨‹å†…éƒ¨åŠ è½½æ¨¡åž‹ï¼Œé¿å… Pickle é”™è¯¯
        low_model = None
        if low_level_path:
            try:
                # åªéœ€è¦ CPU æŽ¨ç†ï¼Œmap_location='cpu' è‡³å…³é‡è¦
                low_model = SAC.load(low_level_path, device='cpu')
            except Exception as e:
                print(f"âš ï¸ [Worker {rank}] Failed to load low-level model from {low_level_path}: {e}")

        # HighLevelEnv æŽ¥æ”¶ loaded model
        env = HighLevelEnv(cfg, is_eval=False, low_model=low_model)
        env.reset(seed=seed + rank)
        return env

    return _init


class SACTrainer:
    """
    SAC Trainer (Parallelized for Speed)
    """

    def __init__(self, config_path: str, output_dir: str):
        self.cfg = load_config(config_path)
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

        self.logger = get_logger("Trainer", os.path.join(output_dir, "train.log"))

        # ä¿å­˜æœ¬æ¬¡è¿è¡Œçš„é…ç½®å‰¯æœ¬
        with open(os.path.join(output_dir, "config_final.yaml"), "w") as f:
            yaml.dump(self.cfg, f)

        # --- [CRITICAL] å¹¶è¡ŒçŽ¯å¢ƒè®¾ç½® ---
        self.n_envs = self.cfg['training'].get('n_envs', 16)
        seed = self.cfg['training'].get('seed', 42)

        # [æ–°å¢ž] è‡ªåŠ¨å¯»æ‰¾é¢„è®­ç»ƒå¥½çš„ Low-Level æ¨¡åž‹
        run_paths = RunPaths(os.path.dirname(config_path), strategy="low_level")
        low_model_path = run_paths.global_low_level_model_path()

        # æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
        final_low_path_str = None
        if low_model_path.exists():
            self.logger.info(f"âœ… Found Pretrained Low-Level Model: {low_model_path}")
            final_low_path_str = str(low_model_path)
        else:
            self.logger.warning(
                f"âš ï¸ Low-level model NOT found at {low_model_path}. High-level training will use fallback (Naive) logic!")

        self.logger.info(f"âš¡ Initializing {self.n_envs} Parallel Environments (SubprocVecEnv)...")

        # [ä¿®æ”¹] åˆ›å»ºå¹¶è¡ŒçŽ¯å¢ƒæ—¶ä¼ å…¥è·¯å¾„
        self.env = SubprocVecEnv([
            make_parallel_env(self.cfg, i, seed, low_level_path=final_low_path_str)
            for i in range(self.n_envs)
        ])

        self.env = VecMonitor(self.env, filename=os.path.join(output_dir, "monitor.csv"))

        # è¯„ä¼°çŽ¯å¢ƒä¿æŒå•è¿›ç¨‹
        # æ³¨æ„ï¼šè¯„ä¼°çŽ¯å¢ƒä¹Ÿéœ€è¦åŠ è½½ Low-Level æ¨¡åž‹ï¼Œè¿™é‡Œä¸ºäº†ç®€å•ç›´æŽ¥å®žä¾‹åŒ–
        # å¦‚æžœå†…å­˜å…è®¸ï¼Œä¹Ÿå¯ä»¥åŠ è½½ä¸€æ¬¡æ¨¡åž‹å¯¹è±¡ä¼ è¿›åŽ»
        eval_low_model = None
        if final_low_path_str:
            eval_low_model = SAC.load(final_low_path_str, device='cpu')

        self.eval_env = HighLevelEnv(self.cfg, is_eval=True, low_model=eval_low_model)

        # è¾…åŠ©æ¨¡å—
        self.cmdp_manager = ConstraintManager(self.cfg)
        self.reward_norm = RewardNormalizer()

        # åˆå§‹åŒ– SAC æ¨¡åž‹
        agent_cfg = self.cfg['agent']
        self.model = SAC(
            "MlpPolicy",
            self.env,
            gamma=agent_cfg['gamma'],
            learning_rate=agent_cfg['learning_rate'],
            buffer_size=agent_cfg['buffer_size'],
            learning_starts=agent_cfg['learning_starts'],
            batch_size=agent_cfg['batch_size'],
            tau=agent_cfg['tau'],
            ent_coef=agent_cfg['ent_coef'],
            train_freq=agent_cfg['train_freq'],
            gradient_steps=agent_cfg['gradient_steps'],
            policy_kwargs=dict(
                net_arch=agent_cfg['hidden_sizes'],
                features_extractor_class=CustomMarketExtractor,
                features_extractor_kwargs=dict(features_dim=256)
            ),
            verbose=1,
            tensorboard_log=os.path.join(output_dir, "tb_logs"),
            seed=seed
        )

        self.best_eval_score = -np.inf
        self.rolling_score_window = []

    def warmup_buffer(self):
        """
        [å‘é‡åŒ– Warm-up]

        [FIX]:
        ä¹‹å‰ä½¿ç”¨äº† for å¾ªçŽ¯é€ä¸ªæ·»åŠ çŽ¯å¢ƒæ•°æ®ï¼Œå¯¼è‡´äº† 'cannot reshape array' é”™è¯¯ã€‚
        çŽ°åœ¨çš„å®žçŽ°å®Œå…¨ç¬¦åˆ VecEnv çš„ Batch æ“ä½œè§„èŒƒã€‚
        """
        total_warmup_steps = self.cfg['agent'].get('learning_starts', 10000)
        # è¿™é‡Œçš„ step æ˜¯æŒ‡ environment stepsï¼Œå› ä¸ºæ˜¯å¹¶è¡Œçš„ï¼Œæ‰€ä»¥å¾ªçŽ¯æ¬¡æ•°è¦é™¤ä»¥ n_envs
        loops = int(np.ceil(total_warmup_steps / self.n_envs))

        self.logger.info(f"ðŸ”¥ Starting Parallel Warm-up: {loops} loops x {self.n_envs} envs...")

        obs = self.env.reset()

        for _ in range(loops):
            # æž„é€  Batch Action (16, ActionDim)
            actions = np.zeros((self.n_envs, self.env.action_space.shape[0]), dtype=np.float32)

            # æå– SoC (å‡è®¾ index 12 æ˜¯ System SoC)
            sys_socs = obs[:, 12]

            p_idx = self.env.action_space.shape[0] - 3

            # ç®€å•çš„å¯å‘å¼è§„åˆ™ (å‘é‡åŒ–æ“ä½œ)
            mask_high = sys_socs > 0.6
            mask_low = sys_socs < 0.4

            actions[mask_high, p_idx] = 0.5  # Discharge
            actions[mask_low, p_idx] = -0.5  # Charge

            actions[:, p_idx + 1] = -1.0  # 0 Regulation
            actions[:, p_idx + 2] = 1.0  # Max Slack

            # çŽ¯å¢ƒäº¤äº’ (Batch Step)
            next_obs, rewards, dones, infos = self.env.step(actions)

            # [å…³é”®ä¿®å¤] å‘é‡åŒ–å¤„ç† Terminal Observation
            # VecEnv ä¼šè‡ªåŠ¨ Reset ç»“æŸçš„çŽ¯å¢ƒï¼ŒçœŸå®žçš„ next_obs è—åœ¨ infos é‡Œ
            real_next_obs = next_obs.copy()
            for i, done in enumerate(dones):
                if done and infos[i].get("terminal_observation") is not None:
                    real_next_obs[i] = infos[i]["terminal_observation"]

            # [å…³é”®ä¿®å¤] æ‰¹é‡æ·»åŠ è‡³ ReplayBuffer
            # SB3 çš„ add æ–¹æ³•å½“ n_envs > 1 æ—¶ï¼ŒæœŸæœ›è¾“å…¥ shape ä¸º (n_envs, ...)
            self.model.replay_buffer.add(
                obs,
                real_next_obs,
                actions,
                rewards,
                dones,
                infos
            )

            obs = next_obs

        self.logger.info("âœ… Warm-up Complete.")

    def train(self):
        # 1. æ‰§è¡Œ Warm-up
        self.warmup_buffer()

        # 2. å¼€å§‹æ­£å¼è®­ç»ƒ
        total_timesteps = self.cfg['training']['total_timesteps']
        eval_interval = self.cfg['training']['eval_interval']

        callback = CustomCallback(
            self,
            eval_env=self.eval_env,
            eval_freq=eval_interval
        )

        self.logger.info("ðŸš€ Starting Main Training Loop (Multi-Core)...")
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=self.cfg['training']['log_interval']
        )

        self.logger.info("Training Finished.")
        self.save_checkpoint("final_model")

    def save_checkpoint(self, name: str):
        save_path = os.path.join(self.output_dir, name)
        self.model.save(f"{save_path}.zip")

        # [å…³é”®ä¿®å¤] ä»Žå­è¿›ç¨‹èŽ·å–çœŸå®žçš„ Normalizer çŠ¶æ€
        # self.env æ˜¯ VecEnv, get_attr ä¼šè¿”å›žä¸€ä¸ªåˆ—è¡¨ [norm_0, norm_1, ...]
        try:
            # èŽ·å–æ‰€æœ‰å­çŽ¯å¢ƒçš„ Normalizer
            norm_list = self.env.get_attr("reward_normalizer")

            if norm_list and len(norm_list) > 0:
                # ç­–ç•¥ A: ç›´æŽ¥å–ç¬¬ä¸€ä¸ª (å› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œä¸”é€šå¸¸åŒæ­¥)
                # ç­–ç•¥ B: èšåˆæ‰€æœ‰ (æ›´ä¸¥è°¨ï¼Œä½†è¿™é‡Œå–ç¬¬ä¸€ä¸ªé€šå¸¸è¶³å¤Ÿ)
                real_norm = norm_list[0]

                # å°†çœŸå®žçš„ç»Ÿè®¡é‡è¦†ç›–åˆ°ä¸»è¿›ç¨‹çš„ self.reward_norm
                # è¿™æ ·ä¸‹æ¬¡ self.reward_norm å°±æ˜¯æœ‰æ•°æ®çš„äº†
                self.reward_norm = real_norm

                # æ‰“å°æ—¥å¿—ç¡®è®¤
                self.logger.info(f"ðŸ”„ Synced Normalizer from SubprocEnv (Count={real_norm.rms.count:.1f})")
            else:
                self.logger.warning("âš ï¸ Failed to retrieve reward_normalizer from envs.")

        except Exception as e:
            self.logger.error(f"âŒ Error syncing normalizer: {e}")

        # ä¿å­˜
        norm_path = f"{save_path}_normalizer.pkl"
        with open(norm_path, "wb") as f:
            pickle.dump(self.reward_norm, f)  # çŽ°åœ¨ä¿å­˜çš„æ˜¯æœ‰æ•°æ®çš„å¯¹è±¡äº†

        cmdp_path = f"{save_path}_cmdp.pkl"
        with open(cmdp_path, "wb") as f:
            pickle.dump(self.cmdp_manager, f)

        self.logger.info(f"ðŸ’¾ Checkpoint saved: {name}")


class CustomCallback(BaseCallback):
    """
    SB3 å›žè°ƒï¼šå¤„ç†è¯„ä¼°ã€è¯¾ç¨‹æ›´æ–°å’Œæ¨¡åž‹ä¿å­˜
    """

    def __init__(self, trainer_instance, eval_env, eval_freq: int = 10000):
        super().__init__(verbose=1)
        self.trainer = trainer_instance
        self.eval_env = eval_env
        self.eval_freq = eval_freq

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            self._run_evaluation()
        return True

    def _run_evaluation(self):
        avg_reward = 0.0
        n_eval_episodes = 5

        for _ in range(n_eval_episodes):
            obs, _ = self.eval_env.reset()
            done = False
            ep_reward = 0.0
            while not done:
                action, _ = self.trainer.model.predict(obs, deterministic=True)
                obs, reward, term, trunc, _ = self.eval_env.step(action)
                ep_reward += reward
                done = term or trunc
            avg_reward += ep_reward

        avg_reward /= n_eval_episodes

        window = self.trainer.rolling_score_window
        window.append(avg_reward)
        if len(window) > 5:
            window.pop(0)

        rolling_score = np.mean(window)

        self.trainer.logger.info(
            f"ðŸ“ˆ Step {self.num_timesteps}: Eval Reward = {avg_reward:.2f}, Rolling = {rolling_score:.2f}")

        if rolling_score > self.trainer.best_eval_score:
            self.trainer.best_eval_score = rolling_score
            self.trainer.save_checkpoint("best_rolling_model")

        if self.num_timesteps % self.trainer.cfg['training']['save_interval'] == 0:
            self.trainer.save_checkpoint(f"checkpoint_{self.num_timesteps}")